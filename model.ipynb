{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'imblearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m classification_report, roc_auc_score, precision_recall_curve, auc\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimblearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mover_sampling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SMOTE\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Input, Dense, Dropout, concatenate, Reshape, LSTM, Bidirectional, BatchNormalization, Add, Attention\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'imblearn'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, concatenate, Reshape, LSTM, Bidirectional, BatchNormalization, Add, Attention\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import tensorflow as tf\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "class EnhancedADRPredictor:\n",
    "    def __init__(self, drug_name='Paracetamol', max_results=1000):\n",
    "        self.drug_name = drug_name\n",
    "        self.max_results = max_results\n",
    "        self.base_url = \"https://api.fda.gov/drug/event.json\"\n",
    "        self.model = None\n",
    "        self.reaction_encoder = None\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "    def fetch_openfda_data(self):\n",
    "        params = {'search': f'patient.drug.medicinalproduct:\"{self.drug_name}\"',\n",
    "                  'limit': min(1000, self.max_results), 'skip': 0}\n",
    "        all_results = []\n",
    "        max_retries = 3\n",
    "        \n",
    "        for _ in range(max_retries):\n",
    "            try:\n",
    "                response = requests.get(self.base_url, params=params, timeout=15)\n",
    "                response.raise_for_status()\n",
    "                data = response.json()\n",
    "                results = data.get('results', [])\n",
    "                if results:\n",
    "                    all_results.extend(results)\n",
    "                    if len(all_results) >= self.max_results:\n",
    "                        break\n",
    "                params['skip'] += len(results)\n",
    "                print(f\"Fetched {len(all_results)} records so far...\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error fetching data: {e}\")\n",
    "                continue\n",
    "            break\n",
    "\n",
    "        return pd.DataFrame(self._process_results(all_results)) if all_results else self.generate_sample_data()\n",
    "\n",
    "    def _process_results(self, results):\n",
    "        processed = []\n",
    "        for event in results:\n",
    "            try:\n",
    "                reactions = [reac['reactionmeddrapt'] for reac in event.get('patient',{}).get('reaction',[])]\n",
    "                concomitant_drugs = len(event.get('patient', {}).get('drug', []))\n",
    "                \n",
    "                entry = {\n",
    "                    'age': float(event.get('patient',{}).get('patientonsetage',30)),\n",
    "                    'sex': event.get('patient',{}).get('patientsex','unknown').lower(),\n",
    "                    'weight': float(event.get('patient',{}).get('patientweight',70)),\n",
    "                    'dosage': self._extract_dosage(event),\n",
    "                    'reactions': reactions if reactions else ['unknown'],\n",
    "                    'concomitant_drugs': concomitant_drugs if concomitant_drugs > 0 else 1,\n",
    "                    'serious': 1 if any(int(v) for v in event.get('seriousness',{}).values()) else 0\n",
    "                }\n",
    "                processed.append(entry)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing entry: {e}\")\n",
    "        return processed\n",
    "\n",
    "    def _extract_dosage(self, event):\n",
    "        try:\n",
    "            dosage_text = event.get('patient',{}).get('drug',[{}])[0].get('drugdosagetext','')\n",
    "            if 'mg' in dosage_text.lower():\n",
    "                dosage_value = ''.join(filter(str.isdigit, dosage_text.split('mg')[0]))\n",
    "            else:\n",
    "                dosage_value = ''.join(filter(str.isdigit, dosage_text))\n",
    "            return float(dosage_value) if dosage_value and any(c.isdigit() for c in dosage_value) else 1.0\n",
    "        except Exception:\n",
    "            return 1.0\n",
    "\n",
    "    def preprocess_data(self, df):\n",
    "        df['sex'] = df['sex'].map({'male':0, 'female':1}).fillna(0.5)\n",
    "        df['reactions'] = df['reactions'].apply(lambda x: x if isinstance(x, list) else ['unknown'])\n",
    "        \n",
    "        all_reactions = list(set(reac for sublist in df['reactions'] for reac in sublist)) or ['unknown']\n",
    "        self.reaction_encoder = {reac:i+1 for i,reac in enumerate(all_reactions)}\n",
    "        \n",
    "        df['reaction_seq'] = df['reactions'].apply(\n",
    "            lambda x: [self.reaction_encoder.get(r,0) for r in x][:10] + [0]*(10-len(x))\n",
    "        )\n",
    "        \n",
    "        df['reaction_count'] = df['reactions'].apply(len)\n",
    "        df['bmi'] = df['weight'] / ((df['age']/100) ** 2)\n",
    "        df['bmi'] = df['bmi'].clip(15, 45)\n",
    "        \n",
    "        numerical_features = ['age', 'weight', 'dosage', 'concomitant_drugs', 'reaction_count', 'bmi']\n",
    "        df[numerical_features] = df[numerical_features].fillna(df[numerical_features].median())\n",
    "        \n",
    "        df[numerical_features] = self.scaler.fit_transform(df[numerical_features])\n",
    "        \n",
    "        df['target'] = df['serious']\n",
    "        \n",
    "        if df['target'].nunique() < 2:\n",
    "            print(\"Warning: Single-class dataset detected. Regenerating balanced sample data.\")\n",
    "            return self.generate_sample_data(force_balance=True)\n",
    "            \n",
    "        return df\n",
    "\n",
    "    def generate_sample_data(self, num_samples=1000, force_balance=True):\n",
    "        np.random.seed(42)\n",
    "        \n",
    "        serious = np.concatenate([\n",
    "            np.zeros(int(num_samples*0.7)),\n",
    "            np.ones(int(num_samples*0.3))\n",
    "        ]) if force_balance else np.random.choice([0,1], num_samples, p=[0.7,0.3])\n",
    "        \n",
    "        common_reactions = ['headache', 'nausea', 'vomiting', 'dizziness', 'rash', 'fatigue']\n",
    "        serious_reactions = ['anaphylaxis', 'seizure', 'cardiac_arrest', 'liver_failure', 'renal_failure']\n",
    "        \n",
    "        reactions_list = []\n",
    "        for i in range(num_samples):\n",
    "            if serious[i] == 1:\n",
    "                num_reactions = np.random.randint(1, 4)\n",
    "                reactions = list(np.random.choice(serious_reactions, 1))\n",
    "                reactions.extend(list(np.random.choice(common_reactions, num_reactions-1)))\n",
    "            else:\n",
    "                num_reactions = np.random.randint(1, 3)\n",
    "                reactions = list(np.random.choice(common_reactions, num_reactions))\n",
    "            reactions_list.append(reactions)\n",
    "        \n",
    "        return pd.DataFrame({\n",
    "            'age': np.random.randint(18, 80, num_samples),\n",
    "            'sex': np.random.choice(['male','female','unknown'], num_samples),\n",
    "            'weight': np.random.uniform(50, 100, num_samples),\n",
    "            'dosage': np.random.exponential(scale=50, size=num_samples),\n",
    "            'concomitant_drugs': np.random.randint(1, 5, num_samples),\n",
    "            'reactions': reactions_list,\n",
    "            'serious': serious\n",
    "        }).pipe(self.preprocess_data)\n",
    "\n",
    "    def build_hybrid_model(self):\n",
    "        lstm_input = Input(shape=(10,), name='reaction_seq')\n",
    "        x = Reshape((10, 1))(lstm_input)\n",
    "        \n",
    "        # First LSTM layer with batch normalization\n",
    "        lstm1 = Bidirectional(LSTM(128, return_sequences=True))(x)\n",
    "        bn1 = BatchNormalization()(lstm1)\n",
    "        dropout1 = Dropout(0.3)(bn1)\n",
    "        \n",
    "        # Second LSTM layer with attention\n",
    "        lstm2 = Bidirectional(LSTM(64, return_sequences=True))(dropout1)\n",
    "        attention = Attention()([lstm2, lstm2])\n",
    "        flattened = tf.keras.layers.Flatten()(attention)\n",
    "        \n",
    "        # Structured data branch with more features\n",
    "        struct_input = Input(shape=(6,), name='structured_features')\n",
    "        y = Dense(64, activation='relu', kernel_regularizer=l2(0.001))(struct_input)\n",
    "        y = BatchNormalization()(y)\n",
    "        y = Dropout(0.3)(y)\n",
    "        y = Dense(32, activation='relu', kernel_regularizer=l2(0.001))(y)\n",
    "        \n",
    "        # Combine branches\n",
    "        combined = concatenate([flattened, y])\n",
    "        z = Dense(64, activation='relu', kernel_regularizer=l2(0.001))(combined)\n",
    "        z = BatchNormalization()(z)\n",
    "        z = Dropout(0.3)(z)\n",
    "        z = Dense(32, activation='relu', kernel_regularizer=l2(0.001))(z)\n",
    "        z = Dropout(0.2)(z)\n",
    "        \n",
    "        output = Dense(1, activation='sigmoid')(z)\n",
    "        \n",
    "        self.model = Model(inputs=[lstm_input, struct_input], outputs=output)\n",
    "        \n",
    "        # Use focal loss for better handling of class imbalance\n",
    "        def focal_loss(gamma=2., alpha=.25):\n",
    "            def focal_loss_fixed(y_true, y_pred):\n",
    "                pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "                pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "                return -tf.reduce_mean(alpha * tf.pow(1. - pt_1, gamma) * tf.math.log(pt_1 + 1e-7)) - \\\n",
    "                       tf.reduce_mean((1 - alpha) * tf.pow(pt_0, gamma) * tf.math.log(1. - pt_0 + 1e-7))\n",
    "            return focal_loss_fixed\n",
    "        \n",
    "        self.model.compile(\n",
    "            optimizer=RMSprop(learning_rate=0.001),\n",
    "            loss=focal_loss(gamma=2.0, alpha=0.25),\n",
    "            metrics=['accuracy', tf.keras.metrics.AUC(name='auc'), tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
    "        )\n",
    "\n",
    "    def train_model(self, df):\n",
    "        X_seq = np.array(df['reaction_seq'].tolist())\n",
    "        X_struct = df[['age', 'weight', 'dosage', 'concomitant_drugs', 'reaction_count', 'bmi']].values\n",
    "        y = df['target'].values\n",
    "        \n",
    "        unique_classes, counts = np.unique(y, return_counts=True)\n",
    "        print(f\"Class distribution: {dict(zip(unique_classes, counts))}\")\n",
    "        \n",
    "        if len(unique_classes) < 2:\n",
    "            raise ValueError(\"Insufficient classes for training. Need at least 2 classes.\")\n",
    "        \n",
    "        # Use stratified k-fold cross-validation for more robust evaluation\n",
    "        n_splits = 5\n",
    "        skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "        fold_metrics = []\n",
    "        \n",
    "        for fold, (train_idx, test_idx) in enumerate(skf.split(X_struct, y)):\n",
    "            print(f\"\\nTraining fold {fold+1}/{n_splits}\")\n",
    "            \n",
    "            X_train_seq, X_test_seq = X_seq[train_idx], X_seq[test_idx]\n",
    "            X_train_struct, X_test_struct = X_struct[train_idx], X_struct[test_idx]\n",
    "            y_train, y_test = y[train_idx], y[test_idx]\n",
    "            \n",
    "            # Apply SMOTE to balance classes\n",
    "            smote = SMOTE(random_state=42)\n",
    "            X_train_combined = np.concatenate([X_train_seq, X_train_struct], axis=1)\n",
    "            X_train_combined_resampled, y_train_resampled = smote.fit_resample(X_train_combined, y_train)\n",
    "            \n",
    "            X_train_seq_resampled = X_train_combined_resampled[:, :10]\n",
    "            X_train_struct_resampled = X_train_combined_resampled[:, 10:]\n",
    "            \n",
    "            # Build a fresh model for each fold\n",
    "            self.build_hybrid_model()\n",
    "            \n",
    "            # Train with callbacks\n",
    "            history = self.model.fit(\n",
    "                [X_train_seq_resampled, X_train_struct_resampled], \n",
    "                y_train_resampled,\n",
    "                validation_data=([X_test_seq, X_test_struct], y_test),\n",
    "                epochs=100,\n",
    "                batch_size=32,\n",
    "                callbacks=[\n",
    "                    EarlyStopping(monitor='val_auc', patience=15, mode='max', restore_best_weights=True),\n",
    "                    ReduceLROnPlateau(monitor='val_auc', factor=0.5, patience=5, mode='max'),\n",
    "                    ModelCheckpoint(f'adr_model_fold_{fold}.h5', monitor='val_auc', mode='max', save_best_only=True)\n",
    "                ],\n",
    "                verbose=1\n",
    "            )\n",
    "            \n",
    "            # Evaluate on test set\n",
    "            y_pred_proba = self.model.predict([X_test_seq, X_test_struct])\n",
    "            y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            fold_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "            precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "            fold_auprc = auc(recall, precision)\n",
    "            \n",
    "            print(f\"\\nFold {fold+1} Classification Report:\")\n",
    "            print(classification_report(y_test, y_pred))\n",
    "            print(f\"AUC-ROC: {fold_auc:.4f}, AUPRC: {fold_auprc:.4f}\")\n",
    "            \n",
    "            fold_metrics.append({\n",
    "                'auc': fold_auc,\n",
    "                'auprc': fold_auprc,\n",
    "                'history': history.history\n",
    "            })\n",
    "        \n",
    "        # Average metrics across folds\n",
    "        avg_auc = np.mean([m['auc'] for m in fold_metrics])\n",
    "        avg_auprc = np.mean([m['auprc'] for m in fold_metrics])\n",
    "        print(f\"\\nAverage AUC-ROC across {n_splits} folds: {avg_auc:.4f}\")\n",
    "        print(f\"Average AUPRC across {n_splits} folds: {avg_auprc:.4f}\")\n",
    "        \n",
    "        # Plot learning curves\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        for i, m in enumerate(fold_metrics):\n",
    "            plt.plot(m['history']['val_auc'], label=f'Fold {i+1}')\n",
    "        plt.title('Validation AUC-ROC')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('AUC-ROC')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        for i, m in enumerate(fold_metrics):\n",
    "            plt.plot(m['history']['val_loss'], label=f'Fold {i+1}')\n",
    "        plt.title('Validation Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('learning_curves.png')\n",
    "        plt.show()\n",
    "\n",
    "    def explain_predictions(self, df, n_samples=10):\n",
    "        sample_data = df.sample(n_samples)\n",
    "        X_seq = np.array(sample_data['reaction_seq'].tolist())\n",
    "        X_struct = sample_data[['age', 'weight', 'dosage', 'concomitant_drugs', 'reaction_count', 'bmi']].values\n",
    "        \n",
    "        # Generate SHAP explanations\n",
    "        explainer = shap.DeepExplainer(self.model, [X_seq, X_struct])\n",
    "        shap_values = explainer.shap_values([X_seq, X_struct])\n",
    "        \n",
    "        # Plot SHAP summary\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        shap.summary_plot(\n",
    "            shap_values[0], \n",
    "            X_struct,\n",
    "            feature_names=['Age', 'Weight', 'Dosage', 'Concomitant Drugs', 'Reaction Count', 'BMI']\n",
    "        )\n",
    "        plt.savefig('shap_summary.png')\n",
    "        plt.show()\n",
    "        \n",
    "        # Create feature importance heatmap\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        feature_importance = np.abs(shap_values[0]).mean(axis=0)\n",
    "        sns.heatmap(\n",
    "            feature_importance.reshape(1, -1), \n",
    "            annot=True, \n",
    "            xticklabels=['Age', 'Weight', 'Dosage', 'Concomitant Drugs', 'Reaction Count', 'BMI'],\n",
    "            yticklabels=['Importance'],\n",
    "            cmap='viridis'\n",
    "        )\n",
    "        plt.title('Feature Importance Heatmap')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('feature_importance.png')\n",
    "        plt.show()\n",
    "\n",
    "    def full_pipeline(self):\n",
    "        print(\"Starting enhanced ADR detection pipeline...\")\n",
    "        try:\n",
    "            df = self.fetch_openfda_data()\n",
    "            df = self.preprocess_data(df)\n",
    "            self.train_model(df)\n",
    "            self.explain_predictions(df)\n",
    "            \n",
    "            # Save the final model\n",
    "            self.model.save('adr_final_model.h5')\n",
    "            print(\"Model saved successfully!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Pipeline failed: {str(e)}\")\n",
    "            print(\"Regenerating with synthetic data...\")\n",
    "            df = self.generate_sample_data(force_balance=True)\n",
    "            self.train_model(df)\n",
    "            self.explain_predictions(df)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    predictor = EnhancedADRPredictor(drug_name=\"Aspirin\", max_results=5000)\n",
    "    predictor.full_pipeline()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
